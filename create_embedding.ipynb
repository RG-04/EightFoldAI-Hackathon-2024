{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force TensorFlow to use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract unique words from messages\n",
    "def extract_unique_words(messages):\n",
    "    words = ' '.join(messages).split()  # Combine all messages and split into words\n",
    "    return set(words)  # Return unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataset by species and apply the function to extract unique words\n",
    "species_groups = df.groupby('species')['message']\n",
    "species_unique_words = species_groups.apply(extract_unique_words)\n",
    "\n",
    "# Convert the result into a DataFrame for better readability\n",
    "unique_words_df = pd.DataFrame({\n",
    "    'species': species_unique_words.index,\n",
    "    'unique_words': species_unique_words.values\n",
    "})\n",
    "\n",
    "# Reset the index to make it cleaner\n",
    "unique_words_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(unique_words_df)\n",
    "unique_words_df.to_csv('unique_words.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_words(species_list):\n",
    "    # Get the unique words for the first species in the list\n",
    "    common_words = species_unique_words[species_list[0]]\n",
    "    \n",
    "    # Intersect the words with the rest of the species in the list\n",
    "    for species in species_list[1:]:\n",
    "        common_words = common_words.intersection(species_unique_words[species])\n",
    "    \n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_different_words(species1, species2):\n",
    "    # Get the unique words for the first species in the list\n",
    "    different_words = species_unique_words[species1]\n",
    "\n",
    "    # Find the difference between the words of the first species and the second species\n",
    "    different_words = different_words.difference(species_unique_words[species2])\n",
    "    \n",
    "    return different_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs = [('Aquari', 'Florian'), ('Zorblax', 'Quixnar'), ('Faerix', 'Mythron'), ('Nexoon', 'Cybex'), ('Emotivor', 'Sentire')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = df['species'].unique()\n",
    "species_combinations = list(combinations(species, 2))\n",
    "common_words = {}\n",
    "\n",
    "for species_combination in species_combinations:\n",
    "    com_words = find_common_words(species_combination)\n",
    "    if len(com_words) > 0:\n",
    "        common_words[species_combination[0]] = com_words\n",
    "        common_words[species_combination[1]] = com_words\n",
    "\n",
    "common_words_df = pd.DataFrame({\n",
    "    'species_combination': common_words.keys(),\n",
    "    'common_words': common_words.values()\n",
    "})\n",
    "\n",
    "common_words_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "common_words_df.to_csv('common_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m different_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m species_combination \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpairs\u001b[49m:\n\u001b[0;32m      4\u001b[0m     different_words[species_combination[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m find_different_words(species_combination[\u001b[38;5;241m0\u001b[39m], species_combination[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m     different_words[species_combination[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m find_different_words(species_combination[\u001b[38;5;241m1\u001b[39m], species_combination[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pairs' is not defined"
     ]
    }
   ],
   "source": [
    "# different_words = {}\n",
    "\n",
    "# for species_combination in pairs:\n",
    "#     different_words[species_combination[0]] = find_different_words(species_combination[0], species_combination[1])\n",
    "#     different_words[species_combination[1]] = find_different_words(species_combination[1], species_combination[0])\n",
    "\n",
    "# different_words = pd.DataFrame({\n",
    "#     'species_combination': different_words.keys(),\n",
    "#     'different_words': different_words.values()\n",
    "# })\n",
    "\n",
    "# different_words.to_csv('different_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for spec_unique_words in species_unique_words.values:\n",
    "    all_words = all_words.union(spec_unique_words)\n",
    "\n",
    "all_words = list(all_words)\n",
    "\n",
    "# Tokenize the words\n",
    "tokenizer = LabelEncoder()\n",
    "word_indices = tokenizer.fit_transform(all_words)\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "# Create a dictionary to map words to indices\n",
    "species_word_indices = {species: tokenizer.fit_transform(list(spec_unique_words)) for species, spec_unique_words in species_unique_words.items()}\n",
    "common_words_indices = {species: tokenizer.fit_transform(list(spec_common_words)) for species, spec_common_words in common_words.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    # print(y_pred.shape)\n",
    "    anchor, positive, negative = y_pred[:, 0], y_pred[:, 1], y_pred[:, 2]\n",
    "    pos_dist = K.sum(K.square(anchor - positive), axis=-1)\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=-1)\n",
    "    loss = K.maximum(pos_dist - neg_dist + alpha, 0)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(species_word_indices, common_word_indices):\n",
    "    triplets = []\n",
    "    \n",
    "    for species, word_indices in species_word_indices.items():\n",
    "        # For each word in the species\n",
    "        for word_idx in word_indices:\n",
    "            # Positive: A different word from the same species or a common word\n",
    "            pos_idx = random.choice([idx for idx in word_indices if idx != word_idx])\n",
    "            \n",
    "            # Negative: A word from a different species or a common word\n",
    "            neg_species = random.choice([s for s in species_word_indices if s != species])\n",
    "            neg_idx = random.choice(list(species_word_indices[neg_species]) + list(common_word_indices[species]))\n",
    "            \n",
    "            # Append the triplet (anchor, positive, negative)\n",
    "            triplets.append((word_idx, pos_idx, neg_idx))\n",
    "    \n",
    "    return np.array(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1) (None, 1) (None, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate triplets for training, factoring in common words\n",
    "triplets = create_triplets(species_word_indices, common_words_indices)\n",
    "\n",
    "embedding_dim = 32\n",
    "\n",
    "# Define the embedding model\n",
    "input_layer = Input(shape=(1,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "flatten_layer = Flatten()(embedding_layer)\n",
    "embedding_model = Model(inputs=input_layer, outputs=flatten_layer)\n",
    "\n",
    "# Define inputs for the triplet loss model\n",
    "anchor_input = Input(shape=(1,), name='anchor_input', dtype='int64')\n",
    "positive_input = Input(shape=(1,), name='positive_input', dtype='int64')\n",
    "negative_input = Input(shape=(1,), name='negative_input', dtype='int64')\n",
    "anchor_embedding = embedding_model(anchor_input)\n",
    "positive_embedding = embedding_model(positive_input)\n",
    "negative_embedding = embedding_model(negative_input)\n",
    "\n",
    "print(anchor_input.shape, positive_input.shape, negative_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.config.disable_traceback_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1835\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0828\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5845e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3641e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0000e+00 \n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2461cd81370>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=[anchor_embedding, positive_embedding, negative_embedding])\n",
    "triplet_model.compile(optimizer=Adam(), loss=triplet_loss)\n",
    "\n",
    "# Train the model\n",
    "triplet_model.fit([triplets[:, 0], triplets[:, 1], triplets[:, 2]], np.zeros(len(triplets)), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# # Get the final embeddings\n",
    "embeddings = embedding_model.predict(np.array(tokenizer.fit_transform(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = pd.DataFrame(embeddings, index=all_words)\n",
    "word_embeddings.to_csv('word_embeddings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
